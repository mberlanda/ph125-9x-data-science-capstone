---
title: "MovieLens Analysis"
author: "Mauro Berlanda"
date: "4/5/2020"
output: pdf_document
sansfont: Calibri Light
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, cache = TRUE, fig.align = 'center') 
```

<!-- http://rmarkdown.rstudio.com -->

## Introduction/Overview/Executive Summary

<!-- an introduction/overview/executive summary section that describes the dataset and summarizes the goal of the project and key steps that were performed -->

The goal of this project is to develop an algorithm to predict movie ratings.

This analysis is performed on ([MovieLens](https://grouplens.org/datasets/movielens/10m/)) dataset. It contains 10 millions ratings and 100 000 tags applications applied to 10 000 movies by 72 000 users.

```{r data, include=FALSE, cache=FALSE}
# The following lines assume that you run the MovieLens.R script before
# compiling the markdown. Dumping some intermediary data structures will allow
# to make the md compilation lighter

library(knitr)
library(tidyverse)
library(caret)
library(data.table)
library(gridExtra)
load("movie-data.rda")
```

After downloading the entire dataset, two dataframes are created:

* `edx` which is used to perform the analysis and develop the algorithm to predict ratings
* `validation` which contains the true values of the predictions

```{r edx}
dim(edx)
dim(validation)
colnames(edx)
```

The key steps required to identify an optimal solution are:

  * create a train and a test set partition out of `edx` dataset to start and validate the analysis
  * explore the content of `train_set` and visualize the distribution of the outcome (`rating`) by every feature
  * use `train_set` implement several prediction models (see Method/Analysis section for details)
  * run models and ensembles against `test_set`
  * evaluate the accuracy and the RMSE of each solution based on `test_set` (see Result for the output)
  * report the RMSE against the true values to complete the exercise

## Method/Analysis

<!--  a methods/analysis section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach -->

The process of this analysis includes data wrangling, data exploration and the description of the modeling approach.

### Data Wrangling/Cleaning

The `edx` and `validation` are the result of some manipulation on a downloaded dataset from `grouplens.org`.

1. download the zip file in a temporary directory and unzip it into the workspace directory
2. read the raw data from two `.dat` files creating `ratings` and `movies` dataframes
3. normalize column names and cast value as `numeric` and `character`, then join both datasets into the `movielens` dataframe
4. partition the data in `movielens` to create a train test used for developing the model (`edx`, 90%) and the true values used to calculate the RMSE of the final model (`validation`, 10%)
5. ensure that userId and movieId in validation set are also in edx set

Instead of using all the known observations from `edx` dataset, we create a `train_set` and a `test_set` to continue this analysis as follows:

```{r test_train_sets, eval=FALSE}
set.seed(17, sample.kind="Rounding")
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
temp <- edx[test_index,]

# Ensure that userId and movieId in the test_set are included in the train_set
test_set <- temp %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)
```

We perform our analysis unsing `train_set` and we validate it with `test_set`.

### Data Exploration and Insights

```{r load-explore-data, include=FALSE}
load("./data/explore-dataset.rda")
```

Since our goal is to predict ratings, we start looking at the overall ratings distribution in the `train_set` dataset.

```{r ratings_distribution, echo=FALSE, cache=TRUE, out.height='30%'}
p_rating_distribution
t_rating_summary
```
We notice that whole star ratings are predominant compares to half star rating.

The features we may use in our analysis:
```{r features}
setdiff(colnames(edx), c("rating", "title"))
```


The ratings breakdown by top 10 `genres` is:
```{r ratings_by_genre, echo=FALSE, cache=TRUE, out.height='30%'}
t_ratings_by_genre %>% head(10) %>% kable
p_ratings_by_genre
```

Comparing the distribution of rating by `movieId` and by `userId`

```{r movie_user_dist, echo=FALSE, out.height= '25%'}
grid.arrange(
  grobs = list(
    p_bi+coord_flip() + scale_color_hue(),
    p_bu+coord_flip()
  ), ncol = 2, as.table = FALSE
)
```

Comparing the distribution by the `timestamp`, even if the rating count varies somehow over the time, the average rating seems quite constant.

```{r timestamp_dist, echo=FALSE, out.height='40%'}
grid.arrange(
  grobs = list(
    p_ratings_by_year,
    p_ratings_by_month,
    p_ratings_by_week,
    p_ratings_by_wday
  ), ncol = 2, as.table = FALSE
)
```

We can clearly see that on average ratings is costant across the `month` and the `wday`.
There is some seasonality in the reviews distribution, with a majority of ratings given in the last
quarter of the year.

> This data exploration has been done within the limits of my machine capacity (especially in CPU)
> More powerful machines can help to find out more advanced patterns.
> In particular I would have plotted ratings by userId and movieId, extracted the movie year and visualize its impact.

Out of curiosity, the 10 most rated movies are:
```{r most10_rated_movies, echo=FALSE}
t_most10_rated_movies %>% kable()
```

### Modeling

In the first data exploration we could not find identify a single feature allowing to predict clearly.

This *classification* problem should give as outcome 10 possible ratings:
```{r outcomes}
seq(0.5, 5, 0.5)
```

We will allow to predict a continous outcoime to make any ensemble more accurate.
To transform any prediction to one of the allowed ratings we can just use the following formula

```{r ceiling, eval=FALSE}
n <- 2.7
ceiling(n*2)/2
```

In _HarvardX: PH125.8x_ we have seen that if we want to handle more than two features we can use regression trees or random forrest.

Running a regression tree on all these dimensions was not really helpful:

```{r regression_tree, eval=FALSE }
# Regression tree model
rt_model <- rpart(rating ~ ., data = train_set)
plot(rt_model, margin = 0.1)
text(rt_model, cex = 0.75)

# Warning message:
# In labels.rpart(x, minlength = minlength) :
#   more than 52 levels in a predicting factor, truncated for printout
```


Models such as logistic_regression, lda, qda, loess, knn perform better with at maximum two features.
Instead of * against each feature and evaluate the accuracy
* against every two features and evaluate the accuray


## Results

<!-- a results section that presents the modeling results and discusses the model performance -->

## Conclusion

<!-- a conclusion section that gives a brief summary of the report, its limitations and future work -->
